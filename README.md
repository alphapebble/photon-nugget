# ğŸŒ Private AI Assistant with Solar RAG Augmentation

This project is a fully on-premises Retrieval-Augmented Generation (RAG) chatbot designed for solar energy knowledge queries.  
It combines LanceDB vector retrieval, local embedding generation, and Llama/Mistral model-based generation, served through FastAPI and Gradio UI.

---

## ğŸ“š Features

- ğŸ›¡ï¸ Completely offline, on-premises deployment
- ğŸ” Switchable LLM backend (Ollama, Transformers, OpenAI-ready)
- ğŸ“„ Structured prompt templates using YAML + Jinja2
- ğŸ§© Modular design ready for agentic workflows and LangGraph integration
- ğŸ” LanceDB-powered local vector retrieval
- ğŸ¤– Local inference with Mistral / LLaMA models
- ğŸ“ Solar domain-specific document ingestion and embedding
- ğŸš€ FastAPI backend with Gradio-based chatbot frontend
- ğŸ³ Dockerized setup for reproducible, scalable deployment



---

## ğŸ“‚ Project Structure

```
photon-nugget/
â”œâ”€â”€ ingestion/                # Fetch, parse, and clean solar documents (raw â†’ chunks â†’ vector store)
â”‚   â”œâ”€â”€ fetcher.py            # Download PDFs, HTML pages from various sources
â”‚   â”œâ”€â”€ parser.py             # Extract readable text from documents
â”‚   â”œâ”€â”€ cleaner.py            # Clean, split, and preprocess text into knowledge units
â”‚   â””â”€â”€ ingestion_runner.py   # Orchestrates the full ingestion pipeline
â”œâ”€â”€ retriever/                # Vector search layer (e.g., using LanceDB, FAISS)
â”‚   â””â”€â”€ retriever_lancedb.py  # Store and retrieve documents from LanceDB
â”œâ”€â”€ llm/                      # LLM backends (Ollama, Transformers, etc.)
â”‚   â””â”€â”€ chatbot_engine.py     # Unified interface to load and run local models like Mistral/LLaMA
â”œâ”€â”€ app/                      # FastAPI server and request/response models
â”‚   â””â”€â”€ server.py             # API routes and logic using Pydantic
â”œâ”€â”€ ui/                       # Frontend interface
â”‚   â””â”€â”€ app.py                # Gradio-based UI for user interaction
â”œâ”€â”€ models/                   # Local pre-downloaded models (e.g., mistral-7b-instruct, gemma)
â”œâ”€â”€ data/                     # Input/output artifacts for the pipeline
â”‚   â”œâ”€â”€ lancedb/              # LanceDB vector database files
â”‚   â”œâ”€â”€ docs_raw/             # Raw source documents (e.g., PDFs)
â”‚   â””â”€â”€ docs_clean/           # Cleaned and chunked text for embedding
â”œâ”€â”€ prompts/                  # YAML + Jinja2 structured prompt templates
â”‚   â”œâ”€â”€ solar_rag.prompt      # Example prompt with context and query variables
â”‚   â””â”€â”€ utils/                # Template loader, renderer
â”œâ”€â”€ evaluation/               # (Optional) Evaluation scripts, metrics (BLEU, ROUGE, etc.)
â”œâ”€â”€ requirements.txt          # Python package dependencies
â”œâ”€â”€ Dockerfile                # Container definition for the app
â”œâ”€â”€ docker-compose.yml        # Multi-container orchestration (Ollama + app + DB)
â”œâ”€â”€ download_model.sh         # Script to download and cache model weights
â””â”€â”€ .env                      # Environment variables (MODEL_PATH, USE_OLLAMA, etc.)

```

---

## ğŸ› ï¸ Tech Stack

- **FastAPI** â€” Lightweight Python API backend
- **Gradio** â€” Frontend interface for interactive chat
- **LanceDB** â€” Embedded vector database for fast document retrieval
- **Sentence-Transformers** â€” For generating dense vector embeddings
- **Transformers / Ollama / OpenAI** â€” Switchable LLM backend via unified interface
- **Jinja2 + YAML** â€” Prompt templating system
- **PyMuPDF** â€” Robust PDF parsing for document ingestion
- **Docker** â€” Optional containerization for deployment

---

## ğŸš€ How to Run

### Install dependencies
```bash
pip install -r requirements.txt
```

### Start backend (FastAPI)
```bash
MODEL_PATH=./models/mistral-7b-instruct uvicorn app.server:app --host 0.0.0.0 --port 8000
```

### Start frontend (Gradio)
```bash
MODEL_PATH=./models/mistral-7b-instruct python ui/app.py
```

### Or build and run via Docker
```bash
docker build -t photon-nugget .
docker run -p 8000:8000 -p 8501:8501 -e MODEL_PATH=./models/mistral-7b-instruct photon-nugget
```

---

## ğŸ“š Knowledge Ingestion (Solar Knowledge)

To fetch and store domain-specific solar documents:

```bash
python ingestion/ingestion_runner.py
```

This will:
- Fetch solar installation guides, policies, FAQs
- Parse PDFs and web pages
- Clean text into knowledge units
- Store embeddings into LanceDB

---

## ğŸŒŸ Future Enhancements

- âœ… Add document metadata tracking (source, date)
- âœ… Re-rank retrieved documents by relevance
- âœ… Add answer summarization module
- âœ… Integrate multi-turn conversation memory
- âœ… Auto-schedule document refresh and re-ingestion

---

# ğŸš€ About

This project demonstrates a full **offline RAG pipeline** using open-source components  
for secure, private, knowledge-grounded chatbots in the solar energy domain.

---

# ğŸ›¡ï¸ License

MIT License or specify if using specific restricted datasets.