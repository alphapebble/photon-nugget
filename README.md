# ğŸŒ Private AI Assistant with Solar RAG Augmentation

This project is a fully on-premises Retrieval-Augmented Generation (RAG) chatbot designed for solar energy knowledge queries.  
It combines LanceDB vector retrieval, local embedding generation, and Llama/Mistral model-based generation, served through FastAPI and Gradio UI.

---

## ğŸ“š Features

- ğŸ›¡ï¸ Completely offline, on-prem deployment
- ğŸ” LanceDB-based local vector retrieval
- ğŸ¤– Local Mistral / Llama model serving
- ğŸ“ Solar domain-specific knowledge ingestion
- ğŸš€ FastAPI backend + Gradio frontend
- ğŸ³ Dockerized deployment for easy scaling

---

## ğŸ“‚ Project Structure

```
private-ai-assistant/
â”œâ”€â”€ ingestion/               # Fetch, parse, clean solar documents
â”‚   â”œâ”€â”€ fetcher.py            # Fetch PDFs, HTML pages
â”‚   â”œâ”€â”€ parser.py             # Extract text from PDFs and web pages
â”‚   â”œâ”€â”€ cleaner.py            # Split and clean text into knowledge units
â”‚   â””â”€â”€ ingestion_runner.py   # Run full ingestion pipeline
â”œâ”€â”€ retriever/                
â”‚   â””â”€â”€ retriever_lancedb.py   # Store and retrieve documents using LanceDB
â”œâ”€â”€ model/
â”‚   â””â”€â”€ chatbot_engine.py      # Load and run local Llama/Mistral model
â”œâ”€â”€ app/
â”‚   â””â”€â”€ server.py              # FastAPI backend API
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py                 # Gradio UI frontend
â”œâ”€â”€ models/                    # Pre-downloaded local models (Llama, Mistral)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lancedb/               # LanceDB vector database files
â”‚   â”œâ”€â”€ docs_raw/              # Raw fetched documents
â”‚   â””â”€â”€ docs_clean/            # Cleaned and processed knowledge text
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Dockerfile to containerize the app
â”œâ”€â”€ docker-compose.yml         # (optional) Docker orchestration
â”œâ”€â”€ download_model.sh          # Script to download model locally
â””â”€â”€ .env                       # Environment variables (MODEL_PATH, etc.)
```

---

## ğŸ› ï¸ Tech Stack

- **FastAPI** â€” API backend
- **Gradio** â€” Chat frontend
- **LanceDB** â€” Local vector database
- **Sentence-Transformers** â€” Text embeddings
- **Transformers / Huggingface** â€” Model loading
- **Docker** â€” Optional containerization for deployment
- **PyMuPDF** â€” PDF parsing for ingestion

---

## ğŸš€ How to Run

### Install dependencies
```bash
pip install -r requirements.txt
```

### Start backend (FastAPI)
```bash
MODEL_PATH=./models/mistral-7b-instruct uvicorn app.server:app --host 0.0.0.0 --port 8000
```

### Start frontend (Gradio)
```bash
MODEL_PATH=./models/mistral-7b-instruct python ui/app.py
```

### Or build and run via Docker
```bash
docker build -t private-ai-chatbot .
docker run -p 8000:8000 -p 8501:8501 -e MODEL_PATH=./models/mistral-7b-instruct private-ai-chatbot
```

---

## ğŸ“š Knowledge Ingestion (Solar Knowledge)

To fetch and store domain-specific solar documents:

```bash
python ingestion/ingestion_runner.py
```

This will:
- Fetch solar installation guides, policies, FAQs
- Parse PDFs and web pages
- Clean text into knowledge units
- Store embeddings into LanceDB

---

## ğŸŒŸ Future Enhancements

- âœ… Add document metadata tracking (source, date)
- âœ… Re-rank retrieved documents by relevance
- âœ… Add answer summarization module
- âœ… Integrate multi-turn conversation memory
- âœ… Auto-schedule document refresh and re-ingestion

---

# ğŸš€ About

This project demonstrates a full **offline RAG pipeline** using open-source components  
for secure, private, knowledge-grounded chatbots in the solar energy domain.

---

# ğŸ›¡ï¸ License

MIT License or specify if using specific restricted datasets.