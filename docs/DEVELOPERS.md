# Developer Documentation for Solar-Sage

This document provides technical details for developers who want to understand, modify, or contribute to the Solar-Sage project.

## ğŸ“‚ Project Structure

```
solar-sage/
â”œâ”€â”€ ingestion/                # Fetch, parse, and clean solar documents (raw â†’ chunks â†’ vector store)
â”‚   â”œâ”€â”€ fetcher.py            # Download PDFs, HTML pages from various sources
â”‚   â”œâ”€â”€ parser.py             # Extract readable text from documents
â”‚   â”œâ”€â”€ cleaner.py            # Clean, split, and preprocess text into knowledge units
â”‚   â””â”€â”€ ingestion_runner.py   # Orchestrates the full ingestion pipeline
â”œâ”€â”€ retriever/                # Vector search layer (e.g., using LanceDB, FAISS)
â”‚   â””â”€â”€ retriever_lancedb.py  # Store and retrieve documents from LanceDB
â”œâ”€â”€ llm/                      # LLM backends (Ollama, Transformers, etc.)
â”‚   â””â”€â”€ llm_factory.py        # Unified interface to load and run local models like Mistral/LLaMA
â”œâ”€â”€ rag/                      # RAG pipeline components
â”‚   â””â”€â”€ rag_engine.py         # Combines retrieval and generation
â”œâ”€â”€ app/                      # FastAPI server and request/response models
â”‚   â”œâ”€â”€ server.py             # API routes and logic using Pydantic
â”‚   â””â”€â”€ prompt.py             # Prompt templates and formatting
â”œâ”€â”€ ui/                       # Frontend interface
â”‚   â”œâ”€â”€ app.py                # Gradio-based UI for user interaction
â”‚   â”œâ”€â”€ api.py                # API communication functions
â”‚   â”œâ”€â”€ config.py             # UI configuration settings
â”‚   â”œâ”€â”€ feedback.py           # Feedback handling
â”‚   â”œâ”€â”€ history.py            # Conversation history management
â”‚   â””â”€â”€ theme.py              # Theme and styling
â”œâ”€â”€ models/                   # Local pre-downloaded models (e.g., mistral-7b-instruct, gemma)
â”œâ”€â”€ data/                     # Input/output artifacts for the pipeline
â”‚   â”œâ”€â”€ lancedb/              # LanceDB vector database files
â”‚   â”œâ”€â”€ docs_raw/             # Raw source documents (e.g., PDFs)
â”‚   â””â”€â”€ docs_clean/           # Cleaned and chunked text for embedding
â”œâ”€â”€ prompts/                  # YAML + Jinja2 structured prompt templates
â”‚   â”œâ”€â”€ solar_rag.prompt      # Example prompt with context and query variables
â”‚   â””â”€â”€ utils/                # Template loader, renderer
â”œâ”€â”€ docs/                     # Documentation
â”‚   â””â”€â”€ images/               # Screenshots and images
â”œâ”€â”€ requirements.txt          # Python package dependencies
â”œâ”€â”€ Dockerfile                # Container definition for the app
â”œâ”€â”€ docker-compose.yml        # Multi-container orchestration (Ollama + app + DB)
â””â”€â”€ .env                      # Environment variables (MODEL_PATH, USE_OLLAMA, etc.)
```

## ğŸ› ï¸ Tech Stack

- **FastAPI** â€” Lightweight Python API backend
- **Gradio** â€” Frontend interface for interactive chat
- **LanceDB** â€” Embedded vector database for fast document retrieval
- **Sentence-Transformers** â€” For generating dense vector embeddings
- **Transformers / Ollama / OpenAI** â€” Switchable LLM backend via unified interface
- **Jinja2 + YAML** â€” Prompt templating system
- **PyMuPDF** â€” Robust PDF parsing for document ingestion
- **Docker** â€” Optional containerization for deployment

## ğŸš€ Development Setup

### Environment Setup

For development, it's recommended to use a virtual environment:

```bash
# Create a virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install development dependencies
pip install -r requirements-dev.txt
```

### Running in Development Mode

For faster development iterations, you can run the backend with auto-reload:

```bash
uvicorn app.server:app --reload --host 0.0.0.0 --port 8000
```

### Code Style and Linting

We use:
- Black for code formatting
- isort for import sorting
- flake8 for linting

Run these tools before submitting a pull request:

```bash
black .
isort .
flake8
```

## ğŸ§ª Testing

### Running Tests

```bash
pytest
```

### Adding Tests

Add new tests in the `tests/` directory, following the existing structure:
- `tests/unit/` for unit tests
- `tests/integration/` for integration tests
- `tests/e2e/` for end-to-end tests

## ğŸ”„ API Reference

### Backend API

The backend API is built with FastAPI and provides the following endpoints:

- `POST /chat` - Send a message to the chatbot
  - Request body: `{"query": "your question here"}`
  - Response: `{"response": "AI response here"}`

### UI Components

The UI is built with Gradio and consists of the following main components:

- `app.py` - Main application file
- `config.py` - Configuration settings
- `api.py` - API communication
- `theme.py` - Theme and styling
- `feedback.py` - Feedback handling
- `history.py` - Conversation history management

## ğŸŒŸ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Run tests: `pytest`
5. Submit a pull request

## ğŸ“š Advanced Topics

### Adding New Models

To add support for a new model:

1. Update `llm/llm_factory.py` to include the new model
2. Add model-specific parameters in `app/config.py`
3. Update the documentation

### Customizing the RAG Pipeline

The RAG pipeline can be customized by:

1. Modifying `rag/rag_engine.py` to change the retrieval or generation logic
2. Updating prompt templates in `prompts/`
3. Adjusting retrieval parameters in `retriever/retriever_lancedb.py`

### Docker Deployment

For production deployment, use Docker:

```bash
# Build the Docker image
docker build -t solar-sage .

# Run the container
docker run -p 8000:8000 -p 8502:8502 solar-sage
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](../LICENSE) file for details.
